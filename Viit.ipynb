{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Viit.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO1dReBICJnq6AVcIXwM3gM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IbtissamSaadi/vit-pytorch/blob/main/Viit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xX6gP74Iwf_i",
        "outputId": "be66aab1-b6fb-40f1-b539-e377ddded1b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vit_pytorch\n",
        "!pip install timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebRoxXhYwmHq",
        "outputId": "de37053d-3b17-42e3-e39a-d041bc886853"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: vit_pytorch in /usr/local/lib/python3.7/dist-packages (0.26.3)\n",
            "Requirement already satisfied: einops>=0.3 in /usr/local/lib/python3.7/dist-packages (from vit_pytorch) (0.3.2)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from vit_pytorch) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from vit_pytorch) (0.11.1+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6->vit_pytorch) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->vit_pytorch) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->vit_pytorch) (1.19.5)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.7/dist-packages (0.4.12)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.11.1+cu111)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.19.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#from tqdm import tqdm\n",
        "import PIL.Image as Image\n",
        "#import seaborn as sns\n",
        "#from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "#from matplotlib import rc\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "from torch.optim import lr_scheduler\n",
        "#from sklearn.model_selection import train_test_split\n",
        "#from sklearn.metrics import confusion_matrix, classification_report\n",
        "#from glob import glob\n",
        "#import shutil\n",
        "import timm\n",
        "from collections import defaultdict\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models, datasets\n",
        "from vit_pytorch import ViT\n",
        "\n",
        "import time\n",
        "\n",
        "#%matplotlib inline\n",
        "#%config InlineBackend.figure_format='retina'\n",
        "#sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
        "#HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
        "#sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
        "#rcParams['figure.figsize'] = 12, 8\n",
        "\n",
        "torch.manual_seed(42)\n",
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#Dpath = '/content/drive/MyDrive/CK+48'\n",
        "#Bs_Train = 100\n",
        "#Bs_Test = 1000\n",
        "\"\"\"\n",
        "tform_mnist = torchvision.transforms.Compose([\n",
        "    #torchvision.transforms.RandomHorizontalFlip(),\n",
        "    #torchvision.transforms.RandomRotation(30),\n",
        "    #torchvision.transforms.CenterCrop((16)),\n",
        "    #torchvision.transforms.RandomAffine(degrees=40, scale=(.3, 1.1), shear=0.15),\n",
        "    #torchvision.transforms.Resize((48,48)),                                          \n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize((0.5,),(0.5,))\n",
        "])\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "path_train = '/content/drive/MyDrive/FER2013 small dataset/train'\n",
        "path_vaild = '/content/drive/MyDrive/FER2013 small dataset/test'\n",
        "#apply multiple transformer after each other\n",
        "transforms_vaild = torchvision.transforms.Compose([\n",
        "                                     torchvision.transforms.Resize((224,)),\n",
        "                                     #torchvision.transforms.Grayscale(),\n",
        "                                     torchvision.transforms.ToTensor(),\n",
        "                                     torchvision.transforms.Normalize((0.5026,), (0.2595,))\n",
        "                                     ])\n",
        "\n",
        "# For training, we add some augmentation. Networks are too powerful and would overfit.\n",
        "transforms_train = torchvision.transforms.Compose([\n",
        "                                      torchvision.transforms.Resize((224,)),\n",
        "                                      #torchvision.transforms.Grayscale(),             \n",
        "                                      torchvision.transforms.RandomHorizontalFlip(),\n",
        "                                      #torchvision.transforms.RandomRotation(30),\n",
        "                                      #torchvision.transforms.CenterCrop((16)),\n",
        "                                      #torchvision.transforms.RandomAffine(degrees=40, scale=(.3, 1.1), shear=0.15),\n",
        "                                      torchvision.transforms.ToTensor(),\n",
        "                                      torchvision.transforms.Normalize((0.5066,), (0.2627,))\n",
        "                                     ])\n",
        "\"\"\"\n",
        "transforms_train = transforms.Compose([\n",
        "         transforms.Grayscale(),#Use ImageFolder to expand to three channels by default, just change it back\n",
        "         transforms.RandomHorizontalFlip(),#Random Flip\n",
        "         transforms.ColorJitter(brightness=0.5, contrast=0.5),#Adjust brightness and contrast randomly\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "transforms_vaild = transforms.Compose([\n",
        "    transforms.Grayscale(),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\"\"\"\n",
        "data_train = torchvision.datasets.ImageFolder(root=path_train,transform=transforms_train)\n",
        "data_vaild = torchvision.datasets.ImageFolder(root=path_vaild,transform=transforms_vaild)\n",
        "print(\"Classes: \")\n",
        "classes = data_train.classes \n",
        "print(classes)\n",
        "train_set = torch.utils.data.DataLoader(dataset=data_train,batch_size=128,shuffle=True)\n",
        "vaild_set = torch.utils.data.DataLoader(dataset=data_vaild,batch_size=128,shuffle=False)\n",
        "\"\"\"\n",
        "orig_set = datasets.ImageFolder(root=Dpath )  #  dataset\n",
        "n = len(orig_set)  # total number of examples\n",
        "print(\"Classes: \")\n",
        "print(orig_set.classes)\n",
        "n_test = int(0.15 * n)  # take ~10% for test\n",
        "test_set = orig_set(test_transform)\n",
        "train_set = orig_set(train_transform)\n",
        "\n",
        "test_set = torch.utils.data.Subset(orig_set, range(n_test))  # take first 30%\n",
        "print(\"test \",len(test_set))\n",
        "train_set = torch.utils.data.Subset(orig_set, range(n_test, n))  # take the rest \n",
        "print(\"train \",len(train_set))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=16, \n",
        "                                          shuffle = True )\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=16, \n",
        "                                          shuffle = False)\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "DOWNLOAD_PATH = '/data/mnist'\n",
        "BATCH_SIZE_TRAIN = 100\n",
        "BATCH_SIZE_TEST = 1000\n",
        "\n",
        "transform_mnist = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "train_set = torchvision.datasets.MNIST(DOWNLOAD_PATH, train=True, download=True,\n",
        "                                       transform=transform_mnist)\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE_TRAIN, shuffle=True)\n",
        "\n",
        "test_set = torchvision.datasets.MNIST(DOWNLOAD_PATH, train=False, download=True,\n",
        "                                      transform=transform_mnist)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE_TEST, shuffle=True)\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "vit_model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
        "\n",
        "for param in vit_model.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "num_inputs = vit_model.head.in_features\n",
        "last_layer = nn.Linear(num_inputs, len(orig_set.classes))\n",
        "vit_model.head = last_layer\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "def create_model(n_classes):\n",
        "  model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
        "  n_features = model.head.in_features\n",
        "  model.head = nn.Linear(n_features, n_classes)\n",
        "  return model\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def train_epoch(\n",
        "  model,\n",
        "  data_loader,\n",
        "  loss_fn,\n",
        "  optimizer,\n",
        "  #scheduler,\n",
        "  n_examples\n",
        "):\n",
        "  model = model.train()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  for inputs, labels in data_loader:\n",
        "    #inputs = inputs.to(device)\n",
        "    #labels = labels.to(device)\n",
        "    #start forward pass\n",
        "    outputs = model(inputs)\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    loss = loss_fn(outputs, labels)\n",
        "    correct_predictions += torch.sum(preds == labels)\n",
        "    losses.append(loss.item())\n",
        "    #backward pass\n",
        "    loss.backward()#calculate gradient\n",
        "    optimizer.step()#update wightes\n",
        "    optimizer.zero_grad()#empty our gradients for the next iteration\n",
        "  #scheduler.step()\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)\n",
        "\n",
        "def eval_model(model, data_loader, loss_fn, n_examples):\n",
        "  model = model.eval()\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  #to track the history for evaluate the model and get the accuracy\n",
        "  #we don't calculate the gradient\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in data_loader:\n",
        "      #inputs = inputs.to(device)\n",
        "      #labels = labels.to(device)\n",
        "      outputs = model(inputs)\n",
        "      #actual predict _ we don't need it, predict\n",
        "      _, preds = torch.max(outputs, dim=1)#choose highest prob for every output and class them to the right class\n",
        "      loss = loss_fn(outputs, labels)\n",
        "      correct_predictions += torch.sum(preds == labels)\n",
        "      losses.append(loss.item())\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)\n",
        "\n",
        "def train_model(model, train_loaders, test_loaders, n_epochs):\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "  #scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "  loss_fn =  nn.CrossEntropyLoss()\n",
        "  history = defaultdict(list)\n",
        "\n",
        "  dataset_sizes_train = len(data_train)\n",
        "  dataset_sizes_test = len(data_vaild)\n",
        "  best_accuracy = 0\n",
        "  for epoch in range(n_epochs):\n",
        "    print(f'Epoch {epoch+1 }/{n_epochs}')\n",
        "    print('-' * 10)\n",
        "    train_acc, train_loss = train_epoch(\n",
        "      model,\n",
        "      train_loaders,\n",
        "      loss_fn,\n",
        "      optimizer,\n",
        "      #scheduler,\n",
        "      dataset_sizes_train\n",
        "    )\n",
        "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "    val_acc, val_loss = eval_model(\n",
        "      model,\n",
        "      test_loaders,\n",
        "      loss_fn,\n",
        "      dataset_sizes_test\n",
        "    )\n",
        "    print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "    print()\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    \n",
        "    if val_loss > 2.50:\n",
        "       break\n",
        "   \n",
        "    if val_acc > best_accuracy:\n",
        "      best_accuracy = val_acc\n",
        "      #es = 0\n",
        "      torch.save(model.state_dict(), 'best_model_state.pth')\n",
        "    #else:\n",
        "     # es += 1\n",
        "      #print(\"Counter {} of 5\".format(es))\n",
        "\n",
        "      #if es > 10:\n",
        "       # print(f'Early stopping with best_acc: {best_accuracy}')\n",
        "        #break\n",
        "  print(f'Best val accuracy: {best_accuracy} , Accuracy: {100 * best_accuracy}')\n",
        "  model.load_state_dict(torch.load('best_model_state.pth'))\n",
        "  return model, history\n",
        "\"\"\"\n",
        "base_model = ViT(image_size=48, patch_size=8, num_classes=7, channels=1,\n",
        "             dim=256, depth=6, heads=8, mlp_dim=256, dropout = 0.1, emb_dropout = 0.1)\n",
        "\"\"\"\n",
        "\n",
        "vit_model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
        "for param in vit_model.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "num_inputs = vit_model.head.in_features\n",
        "vit_model.head = nn.Linear(num_inputs, 7)\n",
        "\n",
        "start_time = time.time()\n",
        "base_model, history = train_model(vit_model, train_set, vaild_set, 30)\n",
        "\n",
        "print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oc5CMhYhwoez",
        "outputId": "93ee2dba-1c7f-42ab-cf9b-3283c6084aef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: \n",
            "['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
            "Epoch 1/30\n",
            "----------\n",
            "Train loss 1.6033198616721414 accuracy 0.46035559827006245\n",
            "Val   loss 1.3646010100841521 accuracy 0.48961937716262977\n",
            "\n",
            "Epoch 2/30\n",
            "----------\n",
            "Train loss 1.211252055384896 accuracy 0.5744834214320038\n",
            "Val   loss 1.3198442041873932 accuracy 0.48875432525951557\n",
            "\n",
            "Epoch 3/30\n",
            "----------\n",
            "Train loss 1.0866859826174649 accuracy 0.6086016338298895\n",
            "Val   loss 1.1976202100515365 accuracy 0.5536332179930796\n",
            "\n",
            "Epoch 4/30\n",
            "----------\n",
            "Train loss 1.0261591510339216 accuracy 0.6280634310427679\n",
            "Val   loss 1.1880554020404817 accuracy 0.5493079584775087\n",
            "\n",
            "Epoch 5/30\n",
            "----------\n",
            "Train loss 0.990105253277403 accuracy 0.6427198462277751\n",
            "Val   loss 1.186186710000038 accuracy 0.53719723183391\n",
            "\n",
            "Epoch 6/30\n",
            "----------\n"
          ]
        }
      ]
    }
  ]
}